# -*- coding: utf-8 -*-
"""SCAR_RPNI_size_capped_demo

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tXWtGOQcFbzp33xeOXI6AQ3InI-AyYp9

# SCAR‑RPNI: Size‑Capped, Accuracy‑Oriented RPNI

**Goal.** Learn a DFA from labeled strings with a **strict upper bound** $M$ on the number of states, while **maximizing training accuracy** subject to that bound (with no guarantees).

This notebook implements a practical, self‑contained variant of RPNI (state‑merging) that:

- Enforces a **hard cap** on DFA size (excluding the sink by default).
- First performs **zero‑error merges** (classic RPNI behavior) to get “free” compression.
- Then shrinks further with **error‑aware merges** chosen by a greedy score: _errors added per state removed_.

You can tune $M$, try different target languages, or load your own dataset.

## How to use

1. **Run all** cells once.
2. In the **Playground** section, set the size cap `M`, dataset size, and target language; then run the cell.
3. Inspect the reported **#states**, **training accuracy**, and the **accuracy vs. size** plot.
4. Optionally, upload your own CSV with columns `string,label` (1 for positive, 0 for negative) and re‑run the learning cell.

> **Convention:** The size cap counts **non‑sink** states only (typical in grammar inference). You can include the sink in the count by setting `include_sink_in_count=True` when calling the learner.
"""

import random
import itertools
from collections import deque, defaultdict
from typing import List, Tuple, Dict, Set
import matplotlib.pyplot as plt

random.seed(0)

"""
## Minimal DFA utilities (targets & evaluation)

We provide a few small **target DFAs** to synthesize labeled data, plus helpers to sample strings and evaluate DFAs.
"""

class DFA:
    def __init__(self, alphabet: List[str], start: int, delta: Dict[Tuple[int, str], int], accepting: Set[int]):
        self.alphabet = list(alphabet)
        self.start = start
        self.delta = dict(delta)
        self.accepting = set(accepting)
        # Ensure totality with a sink
        states = {start} | {p for (p, _), _ in self.delta.items()} | set(self.accepting) | set(self.delta.values())
        sink = max(states) + 1 if states else 0
        # Add sink transitions
        for q in (states | {sink}):
            for a in self.alphabet:
                if (q, a) not in self.delta:
                    self.delta[(q, a)] = sink
        for a in self.alphabet:
            self.delta[(sink, a)] = sink
        self.sink = sink
        self.states = {start} | {p for (p, _), _ in self.delta.items()} | set(self.accepting) | set(self.delta.values()) | {sink}

    def accepts(self, s: str) -> bool:
        q = self.start
        for ch in s:
            if ch not in self.alphabet:
                q = self.sink
                break
            q = self.delta[(q, ch)]
        return q in self.accepting

def contains_ab_dfa(alphabet=('a','b')) -> DFA:
    # Language: strings containing substring 'ab'
    alphabet = list(alphabet)
    s0, s1, s2 = 0, 1, 2  # s2 accepting
    delta = {
        (s0, 'a'): s1, (s0, 'b'): s0,
        (s1, 'a'): s1, (s1, 'b'): s2,
        (s2, 'a'): s2, (s2, 'b'): s2,
    }
    return DFA(alphabet, s0, delta, {s2})

def even_as_dfa(alphabet=('a','b')) -> DFA:
    # Language: even number of 'a'
    alphabet = list(alphabet)
    q0, q1 = 0, 1  # q0 accepting
    delta = {}
    for a in alphabet:
        if a == 'a':
            delta[(q0, a)] = q1
            delta[(q1, a)] = q0
        else:
            delta[(q0, a)] = q0
            delta[(q1, a)] = q1
    return DFA(alphabet, q0, delta, {q0})

def ends_with_ab_dfa(alphabet=('a','b')) -> DFA:
    # Language: strings ending with 'ab'
    alphabet = list(alphabet)
    s0, sA, sAB = 0, 1, 2  # sAB accepting
    delta = {
        (s0, 'a'): sA, (s0, 'b'): s0,
        (sA, 'a'): sA, (sA, 'b'): sAB,
        (sAB, 'a'): sA, (sAB, 'b'): s0,
    }
    return DFA(alphabet, s0, delta, {sAB})

def mod3_as_dfa(alphabet=('a','b')) -> DFA:
    # Language: number of 'a' is 0 mod 3
    alphabet = list(alphabet)
    q0, q1, q2 = 0, 1, 2  # q0 accepting
    delta = {}
    for a in alphabet:
        if a == 'a':
            delta[(q0, a)] = q1
            delta[(q1, a)] = q2
            delta[(q2, a)] = q0
        else:
            delta[(q0, a)] = q0
            delta[(q1, a)] = q1
            delta[(q2, a)] = q2
    return DFA(alphabet, q0, delta, {q0})

def sample_strings(alphabet: List[str], n: int, min_len=0, max_len=8, seed=0) -> List[str]:
    rnd = random.Random(seed)
    strings = []
    for _ in range(n):
        L = rnd.randint(min_len, max_len)
        s = ''.join(rnd.choice(alphabet) for _ in range(L))
        strings.append(s)
    return strings

def dataset_from_dfa(dfa: DFA, n=400, min_len=0, max_len=8, seed=0) -> Tuple[List[str], List[str]]:
    X = sample_strings(dfa.alphabet, n, min_len, max_len, seed)
    S_pos, S_neg = [], []
    for s in X:
        if dfa.accepts(s):
            S_pos.append(s)
        else:
            S_neg.append(s)
    return S_pos, S_neg

"""
## APTA and Union‑Find with rollback

We build the **APTA** (prefix tree acceptor) from the labeled sample and maintain a **disjoint‑set (union‑find) with rollback** to evaluate/commit merges efficiently.

- Each union‑find set corresponds to a **DFA state** in the current partition.
- For each set $C$, we track `pos_end[C]` and `neg_end[C]` (how many positive/negative training strings **end** in any PTA node inside $C$).
- The training error contributed by $C$ is `min(pos_end[C], neg_end[C])` (majority vote is optimal per state).
"""

def build_apta(S_pos: List[str], S_neg: List[str], alphabet: List[str]):
    # Build nodes (0 = root)
    delta = []  # list[dict[sym -> next_id]]
    delta.append({})
    pos_end_node = [0]
    neg_end_node = [0]

    # Helper to ensure transition
    def ensure_edge(u, a):
        if a in delta[u]:
            return delta[u][a]
        v = len(delta)
        delta[u][a] = v
        delta.append({})
        pos_end_node.append(0)
        neg_end_node.append(0)
        return v

    # Insert positives
    for s in S_pos:
        u = 0
        for ch in s:
            if ch not in alphabet:
                # Treat unknown symbol as going to sink later
                # For now, just skip creating edge; we'll complete later.
                continue
            u = ensure_edge(u, ch)
        pos_end_node[u] += 1

    # Insert negatives
    for s in S_neg:
        u = 0
        for ch in s:
            if ch not in alphabet:
                continue
            u = ensure_edge(u, ch)
        neg_end_node[u] += 1

    # Add sink and complete to sink
    sink = len(delta)
    delta.append({})  # sink row
    pos_end_node.append(0)
    neg_end_node.append(0)

    for u in range(len(delta)):
        for a in alphabet:
            if a not in delta[u]:
                delta[u][a] = sink

    for a in alphabet:
        delta[sink][a] = sink

    return delta, 0, sink, pos_end_node, neg_end_node

class DSURollback:
    def __init__(self, n: int, pos_end_node: List[int], neg_end_node: List[int]):
        self.n = n
        self.parent = list(range(n))
        self.size = [1] * n
        self.pos = list(pos_end_node)
        self.neg = list(neg_end_node)
        self.canon = list(range(n))  # map each set to a canonical PTA node
        self._stack = []
        # initial error sum = sum over root sets
        roots = set(range(n))
        self.err_sum = 0
        for r in roots:
            self.err_sum += min(self.pos[r], self.neg[r])

    def find(self, x: int) -> int:
        # No path compression (to support rollback)
        while self.parent[x] != x:
            x = self.parent[x]
        return x

    def checkpoint(self):
        return (len(self._stack), self.err_sum)

    def rollback(self, snap):
        stack_size, err_sum = snap
        while len(self._stack) > stack_size:
            op, idx, val = self._stack.pop()
            if op == 'parent':
                self.parent[idx] = val
            elif op == 'size':
                self.size[idx] = val
            elif op == 'pos':
                self.pos[idx] = val
            elif op == 'neg':
                self.neg[idx] = val
            elif op == 'canon':
                self.canon[idx] = val
        self.err_sum = err_sum

    def union(self, x: int, y: int) -> int:
        x = self.find(x)
        y = self.find(y)
        if x == y:
            return x
        # ensure x is larger
        if self.size[x] < self.size[y]:
            x, y = y, x

        # Record old values for rollback
        self._stack.append(('parent', y, self.parent[y]))
        self._stack.append(('size', x, self.size[x]))
        self._stack.append(('pos', x, self.pos[x]))
        self._stack.append(('neg', x, self.neg[x]))
        self._stack.append(('canon', x, self.canon[x]))

        # Update error sum: remove old contributions
        pre_err_x = min(self.pos[x], self.neg[x])
        pre_err_y = min(self.pos[y], self.neg[y])
        # Perform union
        self.parent[y] = x
        self.size[x] += self.size[y]
        self.pos[x] += self.pos[y]
        self.neg[x] += self.neg[y]
        # Keep existing canonical (x)
        post_err_x = min(self.pos[x], self.neg[x])
        self.err_sum = self.err_sum - pre_err_x - pre_err_y + post_err_x
        return x

"""
## Merge closure and the size‑capped learner

- **Closure merge:** When merging two states $P,Q$, we must also merge all their **corresponding successors** for every symbol to preserve determinism.
- **Two phases:**
  1. **Zero‑error phase:** keep merging any pairs that do not increase training error (classic RPNI behavior).
  2. **Size‑cap phase:** if we still have more than $M$ states, choose merges that **minimize** `Δerror / states_removed`.
"""

def counted_state_roots(dsu: DSURollback, sink: int) -> Set[int]:
    sink_root = dsu.find(sink)
    roots = set()
    for i in range(dsu.n):
        r = dsu.find(i)
        if r != sink_root:
            roots.add(r)
    return roots

def support_symbols_for_root(dsu: DSURollback, delta, alphabet: List[str], sink: int, root: int) -> Set[str]:
    sink_root = dsu.find(sink)
    can = dsu.canon[root]
    supp = set()
    for a in alphabet:
        t = dsu.find(delta[can][a])
        if t != sink_root:
            supp.add(a)
    return supp

def merge_closure_try_or_apply(dsu: DSURollback, p: int, q: int, delta, alphabet: List[str], sink: int, commit=False):
    snap = dsu.checkpoint()
    sink_root = dsu.find(sink)
    seen = set()
    Q = deque()
    Q.append((p, q))
    states_removed = 0

    while Q:
        u, v = Q.popleft()
        ru = dsu.find(u)
        rv = dsu.find(v)
        if ru == rv:
            continue
        key = (ru, rv) if ru < rv else (rv, ru)
        if key in seen:
            continue
        seen.add(key)

        # Remember canonicals BEFORE union
        cu = dsu.canon[ru]
        cv = dsu.canon[rv]

        # Count state removal among non-sink roots
        if ru != sink_root and rv != sink_root:
            states_removed += 1

        # Union
        new_root = dsu.union(ru, rv)

        # Enqueue successor pairs
        for a in alphabet:
            u_next = delta[cu][a]
            v_next = delta[cv][a]
            Q.append((u_next, v_next))

    delta_err = dsu.err_sum - snap[1]
    if not commit:
        dsu.rollback(snap)
    return delta_err, states_removed

# def learn_dfa_size_capped(S_pos: List[str],
#                           S_neg: List[str],
#                           alphabet: List[str],
#                           M: int,
#                           include_sink_in_count: bool = False,
#                           verbose: bool = True):
#     delta, root, sink, pos_end_node, neg_end_node = build_apta(S_pos, S_neg, alphabet)
#     dsu = DSURollback(len(delta), pos_end_node, neg_end_node)

#     def num_states_now():
#         if include_sink_in_count:
#             # Count all roots (unique) including sink
#             roots = set(dsu.find(i) for i in range(dsu.n))
#             return len(roots)
#         else:
#             return len(counted_state_roots(dsu, sink))

#     # ---- Phase 1: zero-error merges anywhere ----
#     if verbose:
#         print("Phase 1: zero-error merges")
#     improved = True
#     while improved and num_states_now() > M:
#         improved = False
#         roots = list(counted_state_roots(dsu, sink))
#         best = None
#         best_removed = 0
#         # Evaluate all pairs (unordered)
#         for i in range(len(roots)):
#             for j in range(i + 1, len(roots)):
#                 p, q = roots[i], roots[j]
#                 d_err, d_states = merge_closure_try_or_apply(dsu, p, q, delta, alphabet, sink, commit=False)
#                 if d_states <= 0 or d_err != 0:
#                     continue
#                 # tie-breakers: larger d_states, then larger Jaccard support similarity
#                 supp_p = support_symbols_for_root(dsu, delta, alphabet, sink, p)
#                 supp_q = support_symbols_for_root(dsu, delta, alphabet, sink, q)
#                 jacc = len(supp_p & supp_q) / max(1, len(supp_p | supp_q))
#                 key = (d_states, jacc, -min(p, q), -max(p, q))  # deterministic
#                 if best is None or key > best[0]:
#                     best = (key, p, q, d_states)
#                     best_removed = d_states
#         if best is not None:
#             _, p, q, d_states = best
#             merge_closure_try_or_apply(dsu, p, q, delta, alphabet, sink, commit=True)
#             improved = True
#             if verbose:
#                 print(f"  merged (zero-error): {p} ~ {q}  | removed {d_states} state(s), now = {num_states_now()}")

#     # ---- Phase 2: error-aware shrinking ----
#     if num_states_now() > M:
#         if verbose:
#             print("Phase 2: error-aware shrinking to reach cap M")
#     steps = 0
#     while num_states_now() > M:
#         roots = list(counted_state_roots(dsu, sink))
#         best = None
#         for i in range(len(roots)):
#             for j in range(i + 1, len(roots)):
#                 p, q = roots[i], roots[j]
#                 d_err, d_states = merge_closure_try_or_apply(dsu, p, q, delta, alphabet, sink, commit=False)
#                 if d_states <= 0:
#                     continue
#                 score = d_err / max(1, d_states)
#                 # tie-breakers: fewer errors per state, then more states removed,
#                 # then prefer same majority label, then higher structural similarity
#                 # majority labels (pre-merge)
#                 maj_p = 1 if dsu.pos[dsu.find(p)] >= dsu.neg[dsu.find(p)] else 0
#                 maj_q = 1 if dsu.pos[dsu.find(q)] >= dsu.neg[dsu.find(q)] else 0
#                 same_maj = 1 if maj_p == maj_q else 0
#                 supp_p = support_symbols_for_root(dsu, delta, alphabet, sink, p)
#                 supp_q = support_symbols_for_root(dsu, delta, alphabet, sink, q)
#                 jacc = len(supp_p & supp_q) / max(1, len(supp_p | supp_q))
#                 key = (score, -d_states, -same_maj, -jacc, min(p, q), max(p, q))
#                 if best is None or key < best[0]:
#                     best = (key, p, q, d_err, d_states)
#         if best is None:
#             if verbose:
#                 print("  No further merges possible (unexpected).")
#             break
#         _, p, q, d_err, d_states = best
#         merge_closure_try_or_apply(dsu, p, q, delta, alphabet, sink, commit=True)
#         steps += 1
#         if verbose:
#             print(f"  merged (error-aware): {p} ~ {q} | Δerr={d_err}, removed={d_states}, now={num_states_now()}")

#     # ---- Build the learned DFA from the final partition ----
#     # Map each root to a compact index
#     sink_root = dsu.find(sink)
#     roots = set(dsu.find(i) for i in range(dsu.n))
#     # Put sink last for readability
#     ordered_roots = [r for r in roots if r != sink_root] + [sink_root]
#     idx = {r: k for k, r in enumerate(ordered_roots)}

#     # Acceptance by majority vote; sink is forced rejecting
#     accepting = set()
#     for r in ordered_roots:
#         if r == sink_root:
#             continue
#         if dsu.pos[r] >= dsu.neg[r]:
#             accepting.add(idx[r])

#     # Transitions from canonical nodes
#     learned_delta = {}
#     for r in ordered_roots:
#         can = dsu.canon[r]
#         for a in alphabet:
#             t = dsu.find(delta[can][a])
#             learned_delta[(idx[r], a)] = idx[t]

#     learned_start = idx[dsu.find(0)]
#     learned_dfa = DFA(alphabet, learned_start, learned_delta, accepting)

#     # Training accuracy (should equal derived from err_sum)
#     total = sum(dsu.pos[r] + dsu.neg[r] for r in roots)
#     train_errors = dsu.err_sum
#     train_acc = 1.0 if total == 0 else (total - train_errors) / total

#     # Size accounting
#     non_sink_states = len(ordered_roots) - 1
#     size_counted = len(ordered_roots) if include_sink_in_count else non_sink_states

#     return {
#         "dfa": learned_dfa,
#         "non_sink_states": non_sink_states,
#         "total_states_including_sink": len(ordered_roots),
#         "train_accuracy": train_acc,
#         "train_errors": train_errors,
#         "train_total": total
#     }

def learn_dfa_size_capped(S_pos: List[str],
                          S_neg: List[str],
                          alphabet: List[str],
                          M: int,
                          include_sink_in_count: bool = False,
                          verbose: bool = True,
                          # --- NEW FLAGS ---
                          beam_enabled: bool = False,
                          beam_width: int = 3,
                          beam_depth: int = 2,
                          prune_enable: bool = False,
                          prune_same_parent_only: bool = False,
                          prune_min_jaccard: float = 0.0,
                          prune_same_majority_only: bool = False,
                          prune_max_pairs: int = None,
                          prune_apply_to_phase1: bool = True,
                          prune_apply_to_phase2: bool = True):
    """
    Learn a DFA with a strict size cap M (non-sink states by default), maximizing training accuracy.

    New toggles:
      - beam_enabled, beam_width, beam_depth
      - prune_enable and sub-flags:
          same_parent_only, min_jaccard, same_majority_only, max_pairs,
          prune_apply_to_phase1, prune_apply_to_phase2
    """
    # Build APTA and DSU
    delta, root, sink, pos_end_node, neg_end_node = build_apta(S_pos, S_neg, alphabet)
    dsu = DSURollback(len(delta), pos_end_node, neg_end_node)

    # Precompute parents only if any pruning will use it
    parents_by_child = precompute_parents(delta, alphabet, sink) if prune_enable and (prune_apply_to_phase1 or prune_apply_to_phase2) else None

    # Build pruning configuration dicts for each phase
    prune_cfg_phase1 = None
    prune_cfg_phase2 = None
    if prune_enable and prune_apply_to_phase1:
        prune_cfg_phase1 = {
            "same_parent_only": prune_same_parent_only,
            "min_jaccard": prune_min_jaccard,
            "same_majority_only": prune_same_majority_only,
            "max_pairs": prune_max_pairs,
        }
    if prune_enable and prune_apply_to_phase2:
        prune_cfg_phase2 = {
            "same_parent_only": prune_same_parent_only,
            "min_jaccard": prune_min_jaccard,
            "same_majority_only": prune_same_majority_only,
            "max_pairs": prune_max_pairs,
        }

    def num_states_now():
        if include_sink_in_count:
            roots = set(dsu.find(i) for i in range(dsu.n))
            return len(roots)
        else:
            return len(counted_state_roots(dsu, sink))

    # ---- Phase 1: zero-error merges (classic, with optional pruning) ----
    if verbose:
        print("Phase 1: zero-error merges" + (" (pruned)" if prune_cfg_phase1 else ""))

    improved = True
    while improved and num_states_now() > M:
        improved = False
        choice = select_best_zero_error_merge(dsu, delta, alphabet, sink, prune_cfg_phase1, parents_by_child)
        if choice is None and prune_cfg_phase1:
            # Fallback to no pruning if pruned pool is empty
            choice = select_best_zero_error_merge(dsu, delta, alphabet, sink, None, None)
        if choice is None:
            break
        p, q, d_states = choice
        merge_closure_try_or_apply(dsu, p, q, delta, alphabet, sink, commit=True)
        improved = True
        if verbose:
            print(f"  merged (zero-error): {p} ~ {q} | removed {d_states}, now = {num_states_now()}")

    # ---- Phase 2: error-aware shrinking to reach cap ----
    if num_states_now() > M and verbose:
        print("Phase 2: error-aware shrinking to reach cap M")

    while num_states_now() > M:
        if beam_enabled and beam_depth > 0 and beam_width > 1:
            sel = select_best_merge_beam(dsu, delta, alphabet, sink, M, num_states_now,
                                         beam_width, beam_depth, prune_cfg_phase2, parents_by_child)
            mode = "BEAM"
        else:
            sel = select_best_merge_greedy(dsu, delta, alphabet, sink, prune_cfg_phase2, parents_by_child)
            mode = "GREEDY"

        if sel is None:
            if verbose:
                print("  No further merges possible.")
            break

        p, q, d_err, d_states, score = sel
        merge_closure_try_or_apply(dsu, p, q, delta, alphabet, sink, commit=True)
        if verbose:
            print(f"  merged ({mode}): {p} ~ {q} | Δerr={d_err}, removed={d_states}, score={score:.3f}, now={num_states_now()}")

    # ---- Build the learned DFA from the final partition ----
    sink_root = dsu.find(sink)
    roots = set(dsu.find(i) for i in range(dsu.n))
    ordered_roots = [r for r in roots if r != sink_root] + [sink_root]
    idx = {r: k for k, r in enumerate(ordered_roots)}

    # Acceptance by majority vote; sink is forced rejecting
    accepting = set()
    for r in ordered_roots:
        if r == sink_root:
            continue
        if dsu.pos[r] >= dsu.neg[r]:
            accepting.add(idx[r])

    # Transitions from canonical nodes
    learned_delta = {}
    for r in ordered_roots:
        can = dsu.canon[r]
        for a in alphabet:
            t = dsu.find(delta[can][a])
            learned_delta[(idx[r], a)] = idx[t]

    learned_start = idx[dsu.find(0)]
    learned_dfa = DFA(alphabet, learned_start, learned_delta, accepting)

    # Training error/acc
    total = sum(dsu.pos[r] + dsu.neg[r] for r in roots)
    train_errors = dsu.err_sum
    train_acc = 1.0 if total == 0 else (total - train_errors) / total

    non_sink_states = len(ordered_roots) - 1

    return {
        "dfa": learned_dfa,
        "non_sink_states": non_sink_states,
        "total_states_including_sink": len(ordered_roots),
        "train_accuracy": train_acc,
        "train_errors": train_errors,
        "train_total": total
    }

"""## Optimization: Beam Search & Candidate‑Pair Pruning

### Beam search
When the DFA still exceeds the size cap $M$, the algorithm must accept merges that add training errors. The default **greedy** choice picks the merge with the lowest
$$
\text{score} = \frac{\Delta \text{error}}{\max(1,\ \text{states removed})}.
$$

However, the best *first* merge may only become apparent after looking ahead a few steps.  

**Beam search** explores the top‑$k$ merges at each level for a small depth $L$, evaluating short plans and committing the first move of the best plan.

- **Flags:**  
  `beam_enabled` (bool), `beam_width` (k), `beam_depth` (L).
- **Objective used inside the beam:** minimize the **final training error** after the lookahead; ties break by removing more states and by lower average score.

> Beam search is off by default. Start with `beam_width=3, beam_depth=2` for a mild lookahead.



### Candidate‑pair pruning
On large APTAs, evaluating all $\binom{N}{2}$ pairs per iteration is expensive. Enable **pruning** to shrink the candidate set:

- `prune_same_parent_only`: only consider pairs whose canonical PTA nodes share a **predecessor** in the APTA (locality); fast and very effective.
- `prune_min_jaccard`: require Jaccard similarity ≥ τ between the sets of **defined outgoing symbols** whose transitions stay out of sink—rough structural similarity.
- `prune_same_majority_only`: only consider pairs with the **same current majority label** (both majority‑accepting or both rejecting); prevents obviously costly merges early.
- `prune_max_pairs`: after filtering, cap the number of pairs (random subsample) to keep runtime bounded.
- `prune_apply_to_phase1 / prune_apply_to_phase2`: choose whether to prune in the **zero‑error** phase and/or the **error‑aware** phase.

> Pruning is off by default. A strong, safe starting profile is:
> `prune_enable=True, prune_same_parent_only=True, prune_min_jaccard=0.25, prune_same_majority_only=True, prune_apply_to_phase1=False, prune_apply_to_phase2=True`.

"""

# --- Beam search & candidate-pruning helpers ---

from collections import defaultdict

def precompute_parents(delta, alphabet, sink):
    """For each PTA node v (excl. sink), collect its PTA predecessors (u) across any symbol."""
    parents = [set() for _ in range(len(delta))]
    for u in range(len(delta)):
        if u == sink:
            continue
        row = delta[u]
        for a in alphabet:
            v = row[a]
            if v != sink:
                parents[v].add(u)
    return parents

def majority_label(dsu, r):
    """1 if majority-accepting, 0 otherwise, for the UF root r."""
    return 1 if dsu.pos[r] >= dsu.neg[r] else dsu.neg[r]

def enumerate_candidate_pairs(dsu, delta, alphabet, sink, prune_cfg=None, parents_by_child=None):
    """
    Produce candidate merge pairs (p, q) of current UF roots (ignores sink).
    Applies optional pruning filters:
      - same_parent_only (locality in APTA, via canonical nodes)
      - min_jaccard (structural similarity)
      - same_majority_only (stability)
      - max_pairs (random cap)
    """
    roots = list(counted_state_roots(dsu, sink))
    pairs = []

    # 1) Locality: restrict to pairs sharing an APTA predecessor
    if prune_cfg and prune_cfg.get("same_parent_only") and parents_by_child is not None:
        parent_to_roots = defaultdict(set)
        for r in roots:
            cn = dsu.canon[r]  # canonical PTA node for this UF root
            for par in parents_by_child[cn]:
                parent_to_roots[par].add(r)
        seen = set()
        for childs in parent_to_roots.values():
            cl = list(childs)
            for i in range(len(cl)):
                for j in range(i + 1, len(cl)):
                    p, q = cl[i], cl[j]
                    if p == q:
                        continue
                    key = (p, q) if p < q else (q, p)
                    if key not in seen:
                        seen.add(key)
                        pairs.append(key)
    else:
        # Fallback: all unordered pairs
        for i in range(len(roots)):
            for j in range(i + 1, len(roots)):
                pairs.append((roots[i], roots[j]))

    # 2) Filters: same majority, Jaccard similarity
    out = []
    min_jacc = prune_cfg.get("min_jaccard", 0.0) if prune_cfg else 0.0
    same_maj_only = prune_cfg.get("same_majority_only", False) if prune_cfg else False

    for p, q in pairs:
        rp = dsu.find(p); rq = dsu.find(q)
        if rp == rq:
            continue
        if same_maj_only and majority_label(dsu, rp) != majority_label(dsu, rq):
            continue
        if min_jacc > 0.0:
            supp_p = support_symbols_for_root(dsu, delta, alphabet, sink, rp)
            supp_q = support_symbols_for_root(dsu, delta, alphabet, sink, rq)
            jacc = len(supp_p & supp_q) / max(1, len(supp_p | supp_q))
            if jacc < min_jacc:
                continue
        out.append((rp, rq))

    # 3) Cap the set
    max_pairs = prune_cfg.get("max_pairs") if prune_cfg else None
    if max_pairs is not None and len(out) > max_pairs:
        random.shuffle(out)
        out = out[:max_pairs]

    return out

def select_best_zero_error_merge(dsu, delta, alphabet, sink, prune_cfg=None, parents_by_child=None):
    """Pick the best zero-error merge by tie-breakers (states_removed, Jaccard)."""
    roots = list(counted_state_roots(dsu, sink))
    cand_pairs = enumerate_candidate_pairs(dsu, delta, alphabet, sink, prune_cfg, parents_by_child) if prune_cfg else None
    if not cand_pairs:
        cand_pairs = [(roots[i], roots[j]) for i in range(len(roots)) for j in range(i + 1, len(roots))]

    best = None
    for p, q in cand_pairs:
        d_err, d_states = merge_closure_try_or_apply(dsu, p, q, delta, alphabet, sink, commit=False)
        if d_states <= 0 or d_err != 0:
            continue
        # tie-breakers: more states removed, then higher Jaccard support similarity
        supp_p = support_symbols_for_root(dsu, delta, alphabet, sink, p)
        supp_q = support_symbols_for_root(dsu, delta, alphabet, sink, q)
        jacc = len(supp_p & supp_q) / max(1, len(supp_p | supp_q))
        key = (d_states, jacc, -min(p, q), -max(p, q))
        if best is None or key > best[0]:
            best = (key, p, q, d_states)

    if best is None:
        return None
    _, p, q, d_states = best
    return (p, q, d_states)

def select_best_merge_greedy(dsu, delta, alphabet, sink, prune_cfg=None, parents_by_child=None):
    """Greedy choice in Phase 2: minimize Δerror per state removed."""
    roots = list(counted_state_roots(dsu, sink))
    cand_pairs = enumerate_candidate_pairs(dsu, delta, alphabet, sink, prune_cfg, parents_by_child) if prune_cfg else None
    if not cand_pairs:
        cand_pairs = [(roots[i], roots[j]) for i in range(len(roots)) for j in range(i + 1, len(roots))]

    best = None
    for p, q in cand_pairs:
        d_err, d_states = merge_closure_try_or_apply(dsu, p, q, delta, alphabet, sink, commit=False)
        if d_states <= 0:
            continue
        score = d_err / max(1, d_states)
        # tie-breakers: fewer errors per state, then more states removed,
        # then prefer same majority, then higher structural similarity
        same_maj = 1 if majority_label(dsu, dsu.find(p)) == majority_label(dsu, dsu.find(q)) else 0
        supp_p = support_symbols_for_root(dsu, delta, alphabet, sink, p)
        supp_q = support_symbols_for_root(dsu, delta, alphabet, sink, q)
        jacc = len(supp_p & supp_q) / max(1, len(supp_p | supp_q))
        key = (score, -d_states, -same_maj, -jacc, min(p, q), max(p, q))
        if best is None or key < best[0]:
            best = (key, p, q, d_err, d_states, score)

    if best is None:
        return None
    _, p, q, d_err, d_states, score = best
    return (p, q, d_err, d_states, score)

def select_best_merge_beam(dsu, delta, alphabet, sink, M, num_states_fn, beam_width, beam_depth, prune_cfg=None, parents_by_child=None):
    """
    Depth-L, width-k beam over Phase 2 merges.
    Returns (p, q, d_err, d_states, score) for the FIRST merge of the best plan.
    """

    def eval_branch(depth):
        # returns (final_err_sum, total_states_removed, path_list, sum_scores)
        if num_states_fn() <= M or depth == 0:
            return (dsu.err_sum, 0, [], 0.0)

        roots = list(counted_state_roots(dsu, sink))
        cand_pairs = enumerate_candidate_pairs(dsu, delta, alphabet, sink, prune_cfg, parents_by_child) if prune_cfg else None
        if not cand_pairs:
            cand_pairs = [(roots[i], roots[j]) for i in range(len(roots)) for j in range(i + 1, len(roots))]

        scored = []
        for p, q in cand_pairs:
            d_err, d_states = merge_closure_try_or_apply(dsu, p, q, delta, alphabet, sink, commit=False)
            if d_states <= 0:
                continue
            score = d_err / max(1, d_states)
            same_maj = 1 if majority_label(dsu, dsu.find(p)) == majority_label(dsu, dsu.find(q)) else 0
            supp_p = support_symbols_for_root(dsu, delta, alphabet, sink, p)
            supp_q = support_symbols_for_root(dsu, delta, alphabet, sink, q)
            jacc = len(supp_p & supp_q) / max(1, len(supp_p | supp_q))
            key = (score, -d_states, -same_maj, -jacc, min(p, q), max(p, q))
            scored.append((key, p, q, d_err, d_states, score))

        if not scored:
            return (dsu.err_sum, 0, [], 0.0)

        scored.sort(key=lambda x: x[0])
        to_expand = scored[:max(1, beam_width)]

        best_res = None
        for _, p, q, d_err, d_states, score in to_expand:
            snap = dsu.checkpoint()
            merge_closure_try_or_apply(dsu, p, q, delta, alphabet, sink, commit=True)
            child_final_err, child_removed, child_path, child_sum_scores = eval_branch(depth - 1)
            total_removed = d_states + child_removed
            sum_scores = score + child_sum_scores
            cand = (child_final_err, -total_removed, sum_scores, [(p, q, d_err, d_states)] + child_path)
            if best_res is None or (cand[0], cand[1], cand[2]) < (best_res[0], best_res[1], best_res[2]):
                best_res = cand
            dsu.rollback(snap)

        # unpack best_res
        final_err, neg_removed, sum_scores, path = best_res
        return (final_err, -neg_removed, path, sum_scores)

    final_err, total_removed, path, _ = eval_branch(beam_depth)
    if not path:
        return None
    p, q, d_err, d_states = path[0]
    score = d_err / max(1, d_states) if d_states > 0 else float("inf")
    return (p, q, d_err, d_states, score)

"""
## Experiment harness & plotting

Use `run_experiment` to sweep different size caps $M$ and visualize the training accuracy vs. size trade‑off.
"""

# def run_experiment(S_pos: List[str], S_neg: List[str], alphabet: List[str], Ms: List[int], include_sink_in_count=False, seed=0):
#     results = []
#     for M in Ms:
#         out = learn_dfa_size_capped(S_pos, S_neg, alphabet, M, include_sink_in_count=include_sink_in_count, verbose=False)
#         states = out["total_states_including_sink"] if include_sink_in_count else out["non_sink_states"]
#         results.append((M, states, out["train_accuracy"]))
#         print(f"M={M:>3} | states={states:>3} | train_acc={out['train_accuracy']:.3f}")
#     # Plot
#     Ms_list = [m for m,_,_ in results]
#     accs = [acc for _,_,acc in results]
#     plt.figure()
#     plt.plot(Ms_list, accs, marker='o')
#     plt.xlabel("Size cap M (non-sink states)" if not include_sink_in_count else "Size cap M (including sink)")
#     plt.ylabel("Training accuracy")
#     plt.title("SCAR-RPNI: accuracy vs. size cap")
#     plt.grid(True)
#     plt.show()
#     return results

def run_experiment(S_pos: List[str],
                   S_neg: List[str],
                   alphabet: List[str],
                   Ms: List[int],
                   include_sink_in_count=False,
                   seed=0,
                   **learner_kwargs):
    results = []
    for M in Ms:
        out = learn_dfa_size_capped(
            S_pos, S_neg, alphabet, M,
            include_sink_in_count=include_sink_in_count,
            verbose=False,
            **learner_kwargs
        )
        states = out["total_states_including_sink"] if include_sink_in_count else out["non_sink_states"]
        results.append((M, states, out["train_accuracy"]))
        print(f"M={M:>3} | states={states:>3} | train_acc={out['train_accuracy']:.3f}")
    # Plot
    Ms_list = [m for m, _, _ in results]
    accs = [acc for _, _, acc in results]
    plt.figure()
    plt.plot(Ms_list, accs, marker='o')
    plt.xlabel("Size cap M (non-sink states)" if not include_sink_in_count else "Size cap M (including sink)")
    plt.ylabel("Training accuracy")
    plt.title("SCAR-RPNI: accuracy vs. size cap")
    plt.grid(True)
    plt.show()
    return results

"""
## Demo on synthetic data

We’ll learn from data labeled by a **target DFA** and vary the size cap $M$. Try changing the target (e.g., to `even_as_dfa()`), dataset size, or the range of $M$.
"""

# Choose a target DFA and synthesize data
# alphabet = ['a', 'b']
# # Can also try: even_as_dfa, ends_with_ab_dfa, mod3_as_dfa
# target = contains_ab_dfa(alphabet)
# S_pos, S_neg = dataset_from_dfa(target, n=500, min_len=0, max_len=10, seed=1)

# print(f"Dataset: |S+| = {len(S_pos)}, |S-| = {len(S_neg)}")

# # Sweep different size caps
# Ms = [2, 3, 4, 5, 6, 8, 10]
# results = run_experiment(S_pos, S_neg, alphabet, Ms, include_sink_in_count=False)

# # Train once verbosely at a specific cap to inspect the merge log
# print("\nTraining with detailed logs at M=4:")
# _ = learn_dfa_size_capped(S_pos, S_neg, alphabet, M=4, include_sink_in_count=False, verbose=True)

# """
# ## Playground (edit and re‑run)

# Set `M`, choose a target, and adjust dataset size. Re‑run this cell to see the effect.
# """

# M = 5                     # size cap (non-sink states)

# alphabet = ['a', 'b']
# dataset_size = 600        # number of training strings
# min_len, max_len = 0, 10  # string length range
# seed = 123

# target = ends_with_ab_dfa(alphabet)  # try: contains_ab_dfa, even_as_dfa, mod3_as_dfa
# S_pos, S_neg = dataset_from_dfa(target, n=dataset_size, min_len=min_len, max_len=max_len, seed=seed)

# print(f"Dataset: |S+| = {len(S_pos)}, |S-| = {len(S_neg)}")

# # Beam search options
# beam_enabled = True     # turn beam search on/off
# beam_width   = 4        # k (try 3~5)
# beam_depth   = 2        # L (try 1~3)

# # Pari pruning options
# prune_enable             = True
# prune_same_parent_only   = True
# prune_min_jaccard        = 0.25
# prune_same_majority_only = True
# prune_max_pairs          = 20000     # cap candidate set (or None)
# prune_apply_to_phase1    = False     # keep zero-error phase exhaustive
# prune_apply_to_phase2    = True

# out = learn_dfa_size_capped(
#     S_pos, S_neg, alphabet, M,
#     include_sink_in_count=False,
#     verbose=True,
#     beam_enabled=beam_enabled,
#     beam_width=beam_width,
#     beam_depth=beam_depth,
#     prune_enable=prune_enable,
#     prune_same_parent_only=prune_same_parent_only,
#     prune_min_jaccard=prune_min_jaccard,
#     prune_same_majority_only=prune_same_majority_only,
#     prune_max_pairs=prune_max_pairs,
#     prune_apply_to_phase1=prune_apply_to_phase1,
#     prune_apply_to_phase2=prune_apply_to_phase2
# )
# print(f"\nLearned DFA: states (non-sink) = {out['non_sink_states']}, training accuracy = {out['train_accuracy']:.3f}")

# out = learn_dfa_size_capped(S_pos, S_neg, alphabet, M, include_sink_in_count=False, verbose=True)
# print(f"\nLearned DFA: states (non-sink) = {out['non_sink_states']}, training accuracy = {out['train_accuracy']:.3f}")

"""## Load your own dataset (optional)

Upload a CSV with columns `string,label` (where `label` is `1` for positive, `0` for negative). In Colab, use the file upload widget, then set `csv_path` accordingly.

"""

import csv

def load_csv_dataset(csv_path: str) -> Tuple[List[str], List[str], List[str]]:
    S_pos, S_neg = [], []
    alphabet = set()
    with open(csv_path, newline='') as f:
        reader = csv.DictReader(f)
        for row in reader:
            s = row['string']
            y = int(row['label'])
            for ch in s:
                alphabet.add(ch)
            if y == 1:
                S_pos.append(s)
            else:
                S_neg.append(s)
    return S_pos, S_neg, sorted(alphabet)

# Example usage (uncomment and set path):
# csv_path = '/content/your_dataset.csv'
# S_pos, S_neg, inferred_alphabet = load_csv_dataset(csv_path)
# Ms = [2, 3, 4, 5, 6, 8, 10]
# results = run_experiment(S_pos, S_neg, inferred_alphabet, Ms)

"""
## Notes & tips

- **Counting states.** By default, the size cap excludes the **sink** state. Set `include_sink_in_count=True` to include it.

- **Asymmetric costs.** If false positives/negatives have different cost, replace `min(pos,neg)` with a weighted version in the DSU and adjust the error computation accordingly.
- **Sanity check.** The training accuracy reported equals `1 - (err_sum / total)` where `err_sum` is the sum of per‑state minority counts. That matches majority‑vote labeling per state.
"""